<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="一、人工智能发展简史名词解释 人类智慧：包含了显性智慧和隐性智慧，显性智慧是指人类能够清晰地表达出来的智慧，而隐性智慧是指人类难以清晰地表达出来的智慧。 人类智能：人类智能是指人类在认知、学习、推理、创造、交流、感知、行动等方面所表现出来的能力。一般指显性智慧，是人类智慧的子集。 机器智能：机器智能是计算机（机器）展现出来的智能。 人工智能：可以认为和机器智能等效，是对人类的显性智慧的模拟。 弱人">
<meta property="og:type" content="article">
<meta property="og:title" content="人工智能导论">
<meta property="og:url" content="http://example.com/2023/10/31/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="一、人工智能发展简史名词解释 人类智慧：包含了显性智慧和隐性智慧，显性智慧是指人类能够清晰地表达出来的智慧，而隐性智慧是指人类难以清晰地表达出来的智慧。 人类智能：人类智能是指人类在认知、学习、推理、创造、交流、感知、行动等方面所表现出来的能力。一般指显性智慧，是人类智慧的子集。 机器智能：机器智能是计算机（机器）展现出来的智能。 人工智能：可以认为和机器智能等效，是对人类的显性智慧的模拟。 弱人">
<meta property="og:locale">
<meta property="article:published_time" content="2023-10-31T07:40:59.000Z">
<meta property="article:modified_time" content="2023-11-02T07:53:34.368Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">


<title >人工智能导论</title>

<!-- Favicon -->

    <link href='/logo.png?v=2.1.4' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/logo.png?v=2.1.4' rel='icon' type='image/png' sizes='32x32' ></link>


    <link href='/logo.png?v=2.1.4' rel='apple-touch-icon' sizes='180x180' ></link>


    <link href='/site.webmanifest' rel='manifest' ></link>


<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://npm.elemecdn.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    
<link rel="stylesheet" href="https://npm.elemecdn.com/katex@latest/dist/katex.min.css">





<!-- Icon -->

    
<link rel="stylesheet" href="/css/plugins/font-awesome.min.css">




<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"example.com","author":"John Doe","root":"/","typed_text":["exploring","learning","coding","thinking","creating","sharing","going"],"theme_version":"2.1.4","theme":{"switch":true,"default":"auto"},"favicon":{"logo":"logo.png","icon16":"logo.png","icon32":"logo.png","appleTouchIcon":"logo.png","webmanifest":"/site.webmanifest","visibilitychange":true,"hidden":"failure.ico","showText":"(/≧▽≦/)咦！又好了！","hideText":"(●—●)喔哟，崩溃啦！"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","sticky":"置顶","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":false,"plugin":{"flickr_justified_gallery":"https://npm.elemecdn.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"far fa-sun","moon":"far fa-moon","play":"fas fa-play","email":"far fa-envelope","next":"fas fa-arrow-right","calendar":"far fa-calendar-alt","clock":"far fa-clock","user":"far fa-user","back_top":"fas fa-arrow-up","close":"fas fa-times","search":"fas fa-search","reward":"fas fa-hand-holding-usd","user_tag":"fas fa-user-alt","toc_tag":"fas fa-th-list","read":"fas fa-book-reader","arrows":"fas fa-arrows-alt-h","double_arrows":"fas fa-angle-double-down","copy":"fas fa-copy"},"icontype":"font","highlight":{"plugin":"highlighjs","theme":true,"copy":true,"lang":true,"title":"default","height_limit":false},"toc":{"post_title":true}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2023-11-02 15:53:34"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.1.4" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->
 
<meta name="generator" content="Hexo 6.3.0"></head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
    <div class="trm-holder">
        <div class="preloader">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
</div>
    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><i class="iconfont far fa-sun"></i></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><i class="iconfont far fa-moon"></i></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" style="opacity: 0">
            <!-- top bar -->
            <header class="trm-top-bar">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/logo.png">
    
    
        <div class="trm-logo-text">
            ItoAlgo<span>算法坞</span>
        </div>
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    首页
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/archives/" target="">
                    归档
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/categories/" target="">
                    分类
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/tags/" target="">
                    标签
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/links/" target="">
                    友链
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/about/" target="">
                    关于
                </a>
                
                <ul>
                    
                    <li>
                        <a  href="/about/me/" target="">
                            我的
                        </a>
                    </li>
                    
                </ul>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
    <!-- mode switcher place -->
    <div class="trm-mode-switcher-place">
        <div class="trm-mode-switcher">
            <i class="iconfont far fa-sun"></i>
            <input class="tgl tgl-light" id="trm-swich" type="checkbox">
            <label class="trm-swich" for="trm-swich"></label>
            <i class="iconfont far fa-moon"></i>
        </div>
    </div>
    <!-- mode switcher place end -->

			
		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner">
    
    <!-- banner cover -->
    <img style="object-position:top;object-fit:cover;" alt="banner" class="trm-banner-cover" src="/img/banner.png">
    <!-- banner cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container">
            <div class="row">
                
                <div class="col-lg-4"></div>
                
                <div class="col-lg-8">

                    <!-- banner title -->
                    <div class="trm-banner-text ">
                        <div class="trm-label trm-mb-20">
                            NEWS LETTER
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            人工智能导论
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2023
                                    </span>
                                </li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <span id="scroll-triger" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </span>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
                <div class="trm-page-sidebar col-lg-4 hidden-sm">
                    <!-- main card -->
                    <div class="trm-main-card-frame trm-sidebar">
    <div class="trm-main-card"> 
        <!-- card header -->
<div class="trm-mc-header">
    <div class="trm-avatar-frame trm-mb-20">
        <img alt="Avatar" class="trm-avatar" src="/portrait.jpg">
    </div>
    <h5 class="trm-name trm-mb-15">
        浮芒Friman
    </h5>
    
        <div class="trm-label">
            keep
            <span class="trm-typed-text">
                <!-- Words for theme.user.typedText -->
            </span>
        </div>
    
</div>
<!-- card header end -->
        <!-- sidebar social -->

<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<div class="trm-social">
    
        <a href="https://github.com/Friman04" title="github" rel="nofollow" target="_blank">
            <i class="iconfont fab fa-github"></i>
        </a>
    
        <a href="https://space.bilibili.com/360298272" title="gitee" rel="nofollow" target="_blank">
            <i class="iconfont fab fa-github"></i>
        </a>
    
</div>

<!-- sidebar social end -->
        <!-- info -->
<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<ul class="trm-table trm-mb-20">
    
        <li>
            <div class="trm-label">
                学校:
            </div>
            <div class="trm-label trm-label-light">
                大连理工大学
            </div>
        </li>
    
        <li>
            <div class="trm-label">
                年龄:
            </div>
            <div class="trm-label trm-label-light">
                19
            </div>
        </li>
    
</ul>
<!-- info end -->

        
    <div class="trm-divider trm-mb-40 trm-mt-40"></div>
    <!-- action button -->
    <div class="text-center">
        <a href="mailto:272316212@qq.com" class="trm-btn">
            联系我
            <i class="iconfont far fa-envelope"></i>
        </a>
    </div>
    <!-- action button end -->

    </div>
</div>
                    <!-- main card end -->
                </div>
            
            <div class="trm-page-content col-lg-8">
                <div id="trm-content" class="trm-content">
                    <div class="trm-post-info row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-calendar-alt trm-icon"></i><br>
            10/31
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-clock trm-icon"></i><br>
            15:40
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-user trm-icon"></i><br>
            Friman
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <h1 id="一、人工智能发展简史"><a href="#一、人工智能发展简史" class="headerlink" title="一、人工智能发展简史"></a>一、人工智能发展简史</h1><h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><ul>
<li><strong>人类智慧</strong>：包含了显性智慧和隐性智慧，显性智慧是指人类能够清晰地表达出来的智慧，而隐性智慧是指人类难以清晰地表达出来的智慧。</li>
<li><strong>人类智能</strong>：人类智能是指人类在认知、学习、推理、创造、交流、感知、行动等方面所<strong>表现</strong>出来的能力。一般指显性智慧，是人类智慧的子集。</li>
<li><strong>机器智能</strong>：机器智能是计算机（机器）展现出来的智能。</li>
<li><strong>人工智能</strong>：可以认为和机器智能等效，是对人类的显性智慧的模拟。</li>
<li><strong>弱人工智能</strong>：只专注于解决某一特定任务，比如下棋、图像识别等。目前的人工智能都属于弱人工智能。</li>
<li><strong>强人工智能</strong>：能够像人类一样具有智慧，能够解决各种问题，能够无监督地自我学习、自我进化，与人类进行交互式学习。</li>
<li><strong>超人工智能</strong>：拥有自我思维意识，形成新的智能群体。</li>
</ul>
<h3 id="华为云人工智能解决方案"><a href="#华为云人工智能解决方案" class="headerlink" title="华为云人工智能解决方案"></a>华为云人工智能解决方案</h3><ul>
<li><strong>Ascend</strong>：Ascend是华为自研的AI处理器，专为AI场景设计，包括云端和边缘两种类型，能够提供强大的AI计算能力。</li>
<li><strong>Atlas</strong>：Atlas是华为自研的全场景AI集群，提<strong>供AI计算服务</strong>。</li>
<li><strong>MindSpore</strong>：MindSpore是华为自研的全场景AI<strong>计算框架</strong>，支持云端、边缘和终端等多种场景，与华为Ascend AI处理器配合使用，可以大大提高AI计算的效率。</li>
<li><strong>ModelArts</strong>：ModelArts是华为云的AI开发平台，提供了一站式的AI开发服务，包括数据处理、模型训练、模型部署等功能，可以帮助企业快速开发和部署AI应用。</li>
</ul>
<h3 id="人工智能的三大流派及其代表"><a href="#人工智能的三大流派及其代表" class="headerlink" title="人工智能的三大流派及其代表"></a>人工智能的三大流派及其代表</h3><ul>
<li><p><strong>符号主义</strong>：又称逻辑主义、心理学派或计算机学派。</p>
<p>  符号主义认为，人工智能应该通过操作符号和运行程序来实现。这种观点认为，所有的知识都可以用<strong>符号系统</strong>来表示，而思考就是对符号的操作。</p>
<p>  代表人物有<strong>艾伦·图灵</strong>、<strong>约翰·麦卡锡</strong>、纽厄尔、西蒙、尼尔逊等。</p>
</li>
<li><p><strong>连接主义</strong>：又称为仿生学派或生理学派。</p>
<p>  连接主义主张通过<strong>神经网络</strong>来实现人工智能。这种观点认为，人脑是一个复杂的神经网络，人的智能来自于大量神经元的连接和交互。</p>
<p>  代表人物有麦克洛奇，皮茨，霍普菲尔德，鲁梅尔哈特等。</p>
</li>
<li><p><strong>行为主义</strong>：又称为进化主义或控制论学派。</p>
<p>  行为主义认为，人工智能应该通过模拟人类的<strong>行为</strong>来实现。这种观点认为，智能的本质是行为，而不是思考或知识。</p>
<p>  代表人物有<strong>罗德尼·布鲁克斯</strong>等。</p>
</li>
</ul>
<h3 id="人物"><a href="#人物" class="headerlink" title="人物"></a>人物</h3><ul>
<li><strong>约翰·麦卡锡</strong>：人工智能之父，提出人工智能概念，发明LISP，符号主义的代表人物，提出了“人工智能”这一概念。</li>
<li><strong>艾伦·图灵</strong>：计算机&#x2F;人工智能之父，提出图灵机的概念，提出了“图灵测试”。</li>
<li><strong>马文·明斯基</strong>：框架理论的创立者，人工智能之父。</li>
<li><strong>姚期智</strong>：著名的计算机科学家。他是中国科学院院士、美国科学院院士、美国艺术与科学学院院士。他的研究领域包括复杂性理论、算法理论、量子计算、数据隐私等。2000年依靠密码学研究获得图灵奖。</li>
</ul>
<h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><h4 id="早期（1956年以前）"><a href="#早期（1956年以前）" class="headerlink" title="早期（1956年以前）"></a>早期（1956年以前）</h4><p>1949年，第一台可编程的计算机EDVAC投入使用。</p>
<p>1950年10月，图灵发表论文《计算机器与智能》，探讨什么是人工智能，提出了“图灵测试”。</p>
<p><strong>1956年夏天，麦卡锡等人发起第一届人工智能讨论会。</strong></p>
<h4 id="第一次热潮（1956年-1969年）"><a href="#第一次热潮（1956年-1969年）" class="headerlink" title="第一次热潮（1956年-1969年）"></a>第一次热潮（1956年-1969年）</h4><ul>
<li>人工智能概念被首次提出。</li>
</ul>
<p>1962年，IBM跳棋程序战胜人类高手，同时，随专家系统的出现到达第一次热潮的顶峰。</p>
<p>1969年，召开了第一届国际人工智能联合会议（IJCAI）。</p>
<p>1970年，国际期刊《AI》创刊。</p>
<h4 id="第一次寒冬（1970s）"><a href="#第一次寒冬（1970s）" class="headerlink" title="第一次寒冬（1970s）"></a>第一次寒冬（1970s）</h4><ul>
<li>AI算法达到瓶颈，符号主义无法解决大部分问题。</li>
<li>计算机性能不足，无法处理复杂度为指数及以上的问题，组合优化爆炸。</li>
<li>人们对AI期望过高。</li>
</ul>
<h4 id="第二次热潮（1980s）"><a href="#第二次热潮（1980s）" class="headerlink" title="第二次热潮（1980s）"></a>第二次热潮（1980s）</h4><ul>
<li>利用数据库库建立起专家系统，知识库、知识工程成为主流焦点。</li>
<li>联结主义兴起。</li>
</ul>
<p>1982年，Hopfield发明离散的神经网络模型。</p>
<p>1984年，Hopfield发明连续神经网络模型。</p>
<p>1986年，David Rumelhart，Geoffrey Hinton和Ronald Williams发展了<strong>反向传播算法</strong>，并将其应用于神经网络的训练，这使得反向传播算法得到了广泛的关注和应用。</p>
<h4 id="第二次寒冬（1980s末期）"><a href="#第二次寒冬（1980s末期）" class="headerlink" title="第二次寒冬（1980s末期）"></a>第二次寒冬（1980s末期）</h4><ul>
<li>PC、Mac诞生，通用计算机小型化、开始普及。市场需求被PC取代，政府停止拨款。</li>
<li>专家系统的局限性被发现，无法解决复杂问题，无法处理不确定性。</li>
</ul>
<h4 id="第三次热潮（1990s末）"><a href="#第三次热潮（1990s末）" class="headerlink" title="第三次热潮（1990s末）"></a>第三次热潮（1990s末）</h4><ul>
<li>符号主义退出历史舞台，基于统计模型的技术悄然兴起。</li>
<li>互联网的发展，大数据的出现，使得可用的数据增多。</li>
<li>计算机性能大幅提升，使得大数据的统计计算变为可能。</li>
</ul>
<p>1997年，IBM的深蓝战胜国际象棋世界冠军卡斯帕罗夫。<br>2016年，谷歌的AlphaGo战胜围棋世界冠军李世石。<br>2017年，AlphaGo再次战胜人类顶尖棋手柯洁，彻底超越人类水平。</p>
<h1 id="二、逻辑与推理"><a href="#二、逻辑与推理" class="headerlink" title="二、逻辑与推理"></a>二、逻辑与推理</h1><h2 id="一阶谓词逻辑"><a href="#一阶谓词逻辑" class="headerlink" title="一阶谓词逻辑"></a>一阶谓词逻辑</h2><ul>
<li><p>个体x<sub>1</sub>, x<sub>2</sub>, …：某个独立存在的事物或者某个抽象的概念。</p>
</li>
<li><p>谓词名 P：刻画个体的性质、状态或个体间的关系。</p>
</li>
<li><p>连接词 ¬：表示”非“。</p>
</li>
<li><p>连接词 ∧：表示“且”“与”“合取”。</p>
</li>
<li><p>连接词 ∨：表示“或”“析取”。</p>
</li>
<li><p>连接词 ⇒：表示“蕴含”。（注意，只有P为真、Q为假时，P⇒Q为假）</p>
</li>
<li><p>连接词 ⇔：表示“等价”。</p>
</li>
<li><p>连接词的优先级为：¬ &gt; ∧ &gt; ∨ &gt; ⇒ &gt; ⇔。</p>
</li>
<li><p>量词 ∀：表示“对于所有的”。</p>
</li>
<li><p>量词 ∃：表示“存在”。</p>
</li>
</ul>
<h2 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h2><p>根据前提得出结论的过程。</p>
<ul>
<li>演绎推理：<strong>由一般到特殊</strong>，从判断出发，利用演绎得出结论。</li>
<li>归纳推理：<strong>由特殊到一般</strong>，从足够多的实例推断出总体的规律。</li>
<li>默认推理：在知识不完全的前提下，假设已知进行推理。</li>
</ul>
<p>&#x2F;&#x2F;&#x2F;</p>
<ul>
<li>确定性推理：利用确定性的规则进行推理。</li>
<li>非确定性推理：利用不确定性的规则进行推理。</li>
</ul>
<h3 id="自然演绎推理"><a href="#自然演绎推理" class="headerlink" title="自然演绎推理"></a>自然演绎推理</h3><p>从一组已知为真的事实触发，直接运用经典逻辑的推理规则推出结论的过程。推理过程自然，容易理解，但容易组合爆炸。</p>
<hr>
<p><strong>以下是一道来自Github Copilot Chat的题目：</strong></p>
<blockquote>
<p>假设：</p>
<ol>
<li>所有的程序员都懂得编程。</li>
<li>所有懂得编程的人都可以解决算法问题。</li>
<li>Bob是一个程序员。</li>
</ol>
<p>问题：Bob是否可以解决算法问题？</p>
</blockquote>
<h4 id="解："><a href="#解：" class="headerlink" title="解："></a>解：</h4><ol>
<li>定义谓词<ul>
<li>P(x)表示x是程序员</li>
<li>C(x)表示x懂得编程</li>
<li>S(x)表示x可以解决算法问题</li>
<li>求证：S(Bob)</li>
</ul>
</li>
<li>谓词公式<ul>
<li>由（1），(∀x) P(x) ⇒ C(x)</li>
<li>由（2），(∀x) C(x) ⇒ S(x)</li>
<li>则有(∀x) P(x) ⇒ S(x)</li>
<li>那么P(Bob) ⇒ S(Bob)，即Bob可以解决算法问题。</li>
</ul>
</li>
</ol>
<h3 id="归结演绎推理"><a href="#归结演绎推理" class="headerlink" title="归结演绎推理"></a>归结演绎推理</h3><p>J.A.Robinson在1965年提出归结原理。</p>
<blockquote>
<p>对任一要证明的命题，将其否定作为前提，与已知的事实进行归结，如果归结出空子句，则说明原命题为真。（类似于反证法）</p>
</blockquote>
<ul>
<li>可判定的：存在一个算法，对任意给定的命题，都能够判断其真假。命题公式永真性问题是可判定的。</li>
<li>半可判定的：不存在一个算法，对任意给定的命题，都能够判断其真假。一阶谓词永真性问题是半可判定的。</li>
</ul>
<h4 id="合式公式等价关系"><a href="#合式公式等价关系" class="headerlink" title="合式公式等价关系"></a>合式公式等价关系</h4><ul>
<li><strong>¬(¬P) ⇔ P</strong></li>
<li><strong>P ∨ Q ⇔ ¬P ⇒ Q</strong></li>
<li><strong>德摩根律1：¬(P ∧ Q) ⇔ ¬P ∨ ¬Q</strong></li>
<li><strong>德摩根律2：¬(P ∨ Q) ⇔ ¬P ∧ ¬Q</strong></li>
<li><strong>分配律1：P ∧ (Q ∨ R) ⇔ (P ∧ Q) ∨ (P ∧ R)</strong></li>
<li><strong>分配律2：P ∨ (Q ∧ R) ⇔ (P ∨ Q) ∧ (P ∨ R)</strong></li>
<li><strong>交换律1：P ∧ Q ⇔ Q ∧ P</strong></li>
<li><strong>交换律2：P ∨ Q ⇔ Q ∨ P</strong></li>
<li><strong>结合律1：P ∧ (Q ∧ R) ⇔ (P ∧ Q) ∧ R</strong></li>
<li><strong>结合律2：P ∨ (Q ∨ R) ⇔ (P ∨ Q) ∨ R</strong></li>
<li><strong>逆否律：P ⇒ Q ⇔ ¬P ∨ Q</strong></li>
<li><strong>¬(∃x) P(x) ⇔ (∀x) ¬P(x)</strong></li>
<li><strong>¬(∀x) P(x) ⇔ (∃x) ¬P(x)</strong></li>
<li><strong>(∀x) [P(x) ∧ Q(x)] ⇔ (∀x) P(x) ∧ (∀x) Q(x)</strong></li>
<li><strong>(∃x) [P(x) ∨ Q(x)] ⇔ (∃x) P(x) ∨ (∃x) Q(x)</strong></li>
</ul>
<h4 id="建立子句集"><a href="#建立子句集" class="headerlink" title="建立子句集"></a>建立子句集</h4><ul>
<li>子句：由若干个文字组成的析取式（或）。</li>
<li>合取范式：若干个命题的合取式（且）。</li>
<li>子句集：合取范式下的子命题的集合。</li>
</ul>
<h4 id="归结过程"><a href="#归结过程" class="headerlink" title="归结过程"></a>归结过程</h4><ol>
<li>将命题写成合取范式</li>
<li>求出子句集</li>
<li>对子句集进行归结</li>
<li>如果归结出空子句，则原命题为真；否则，原命题为假。</li>
</ol>
<h4 id="Herbrand定理"><a href="#Herbrand定理" class="headerlink" title="Herbrand定理"></a>Herbrand定理</h4><blockquote>
<p>如果一个合取范式是永真的，那么它的任何一个子句集都是不可满足的。</p>
</blockquote>
<ul>
<li><strong>SKOLEM标准型</strong>：所有量词提前，不含否定词，辖域为整个公式。（即前束范式）</li>
<li><strong>SKOLEM化</strong>：将存在量词转化为函数，去掉全称量词。</li>
</ul>
<h3 id="求取子句集"><a href="#求取子句集" class="headerlink" title="求取子句集"></a>求取子句集</h3><ol>
<li><p>利用<strong>P ∨ Q ⇔ ¬P ⇒ Q消去蕴含符号</strong></p>
</li>
<li><p>利用德摩根定律等<strong>减小否定符号的辖域</strong></p>
</li>
<li><p><strong>标准化变量</strong>，将哑元改名以保证每个量词有自己唯一的哑元</p>
<p> <em>例如(∀x) (P(x) ⇒ (∃x) Q(x))，将后面的x改为y，得到(∀x) (P(x) ⇒ (∃y) Q(y))。</em></p>
</li>
<li><p>对于含有存在量词在全称量词辖域内的公式，需要<strong>消去存在量词</strong>，引入Skolem函数</p>
<p> <em>例如(∀y) [(∃x) P(x, y)]中，x的存在可能依赖y，则引入Skolem函数f(y)，得到(∀y) P(f(y), y)，如果x的存在不依赖y，则引入Skolem常量c，得到(∀y) P(c, y)；</em></p>
<p> <em>(∃x) P(x) ⇒ P(A)。</em></p>
</li>
<li><p><strong>化为前束范式</strong></p>
</li>
<li><p><strong>把母式化为合取范式</strong></p>
</li>
<li><p><strong>消去全程量词</strong></p>
</li>
<li><p>化为子句集，用{A, B}代替{A ∧ B}，<strong>消去连词符号∧</strong>。</p>
</li>
<li><p><strong>更换变量名称</strong></p>
</li>
</ol>
<h3 id="消解推理"><a href="#消解推理" class="headerlink" title="消解推理"></a>消解推理</h3><p>消解推理是一种基于归结原理的推理方法，它的基本思想是，<strong>将两个子句中的文字进行匹配，如果匹配成功，那么就将两个子句中的文字消去，得到一个新的子句</strong>。</p>
<ul>
<li>假言推理：C1：P，C2：P ⇒ Q（¬P ⇒ Q），则C3：Q</li>
<li>合并：C1：P ∨ Q，C2：¬P ∨ Q，则C3：Q</li>
<li>重言式：C1：P ∨ Q，C2：¬P ∨ ¬Q，则C3：∅</li>
<li>矛盾式：C1：P，C2：¬P，则C3：∅</li>
<li>链式：C1：¬P ∨ Q，C2：¬Q ∨ R，则C3：¬P ∨ R</li>
</ul>
<hr>
<p>来点去年真题！</p>
<blockquote>
<p>二、已知 A，B 都是 E 学校的同学，C，D 都是 E 学校的同事，C 是 A 的思政导师，D 是 B 的专业导师，C 是 B 的思政导师。(20 分) </p>
<p>（2）写出一阶谓词表示 A、B、C、D 的关系。</p>
<p>（3）要推出 D 是 A 的专业导师，需要满足什么推断规则。</p>
</blockquote>
<p><strong>解：</strong></p>
<p>（2）</p>
<ol>
<li>定义谓词<ul>
<li>同学：S(x, y)</li>
<li>同事：C(x, y)</li>
<li>思政导师：P(x, y)</li>
<li>专业导师：T(x, y)</li>
</ul>
</li>
<li>谓词公式</li>
</ol>
<p><em><strong>以下不会，懒得写</strong></em></p>
<h3 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h3><p>知识图谱是一种<strong>语义网络</strong>，它是一种<strong>图结构</strong>，由一系列实体和<strong>实体</strong>间的<strong>关系</strong>组成。知识图谱可以用来表示知识，也可以用来进行推理。</p>
<p>一般而言，可将知识图谱中任意两个乡连接点及连接边表示为一个<strong>三元组</strong>，即（实体1，关系，实体2）。例如（北京，首都，中国）。</p>
<h4 id="归纳学习"><a href="#归纳学习" class="headerlink" title="归纳学习"></a>归纳学习</h4><p>归纳逻辑程序设计（Inductive Logic Programming，ILP）是一种基于逻辑的归纳学习方法，它的基本思想是，<strong>从已知的正例和反例中归纳出一般性的规律</strong>。</p>
<h5 id="FOIL算法"><a href="#FOIL算法" class="headerlink" title="FOIL算法"></a>FOIL算法</h5><p>一阶归纳学习（First-Order Inductive Learning，FOIL）通过序贯覆盖实现规则推理。</p>
<p>算法过程：</p>
<ol>
<li>将目标谓词作为所学习推理规则的结论。</li>
<li>将其它谓词逐一作为前提约束谓词加入推理规则，计算FOIL信息增益值，选取最优前提谓词生成新规则，去除不符的规则。</li>
<li>重复2，直到反例不被覆盖。</li>
</ol>
<h4 id="路径排序算法"><a href="#路径排序算法" class="headerlink" title="路径排序算法"></a>路径排序算法</h4><p>路径排序（Path Ranking Algorithm，PRA）是一种用于知识图谱推理的方法。它的基本思想是，通过分析实体之间的路径信息，来预测实体之间可能存在的关系。</p>
<p>路径排序算法的主要步骤包括：</p>
<ol>
<li><strong>路径搜索</strong>：从目标实体开始，按照知识图谱中的关系进行深度优先搜索或广度优先搜索，找出所有可能的路径。</li>
<li><strong>路径特征化</strong>：将找出的路径转化为特征，例如，可以将路径的类型、长度等作为特征。</li>
<li><strong>模型训练</strong>：使用机器学习算法（如逻辑回归、支持向量机等）训练模型，将路径特征作为输入，路径是否正确作为输出。</li>
<li><strong>路径排序</strong>：对于给定的实体对，使用训练好的模型预测每个路径的正确性，然后根据预测的正确性对路径进行排序。</li>
</ol>
<h3 id="不确定性推理"><a href="#不确定性推理" class="headerlink" title="不确定性推理"></a>不确定性推理</h3><p>啊？不考啊？？</p>
<p>那不写了XD</p>
<h1 id="三、-算法"><a href="#三、-算法" class="headerlink" title="三、 算法"></a>三、 算法</h1><h2 id="1-搜索算法"><a href="#1-搜索算法" class="headerlink" title="1. 搜索算法"></a>1. 搜索算法</h2><p>搜索时一种求解问题的一般方法。</p>
<h3 id="搜索的基本概念"><a href="#搜索的基本概念" class="headerlink" title="搜索的基本概念"></a>搜索的基本概念</h3><h4 id="搜索的主要过程："><a href="#搜索的主要过程：" class="headerlink" title="搜索的主要过程："></a>搜索的主要过程：</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">确定初始点，将其作为当前状态。</span><br><span class="line">while loop:</span><br><span class="line">    遍历操作算子集;</span><br><span class="line">    运行操作算子;</span><br><span class="line">    建立指向其父结点的指针;</span><br><span class="line"></span><br><span class="line">    if 生成的新状态不在已生成的状态集中 then</span><br><span class="line">        if 新状态满足问题的约束条件 then</span><br><span class="line">            if 新状态满足问题的目标要求 then</span><br><span class="line">                return;</span><br><span class="line">            else</span><br><span class="line">                将新状态加入已生成的状态集;</span><br><span class="line">        end if</span><br><span class="line">    end if</span><br></pre></td></tr></table></figure>
<h4 id="搜索方向："><a href="#搜索方向：" class="headerlink" title="搜索方向："></a>搜索方向：</h4><ul>
<li>数据驱动：从初始状态出发，向目标状态搜索。</li>
<li>目的驱动：从目标状态出发，向初始状态搜索。</li>
<li>双向搜索：从初始状态和目标状态同时出发，向中间搜索。例如双向BFS。</li>
</ul>
<h4 id="搜索方法："><a href="#搜索方法：" class="headerlink" title="搜索方法："></a>搜索方法：</h4><ul>
<li><strong>盲目搜索</strong>：不考虑问题的特点，只是简单地按照固定的步骤和搜索方向进行搜索。</li>
<li><strong>启发式搜索</strong>：考虑根据问题的特点，利用启发信息来指导搜索方向，以期望更快地找到目标状态。</li>
</ul>
<h4 id="状态空间"><a href="#状态空间" class="headerlink" title="状态空间"></a>状态空间</h4><p>状态空间是一个三元组 (S, F, G) 。</p>
<ul>
<li>S：状态空间，由所有可能的状态组成。其中S<sub>0</sub>为初始状态，S<sub>g</sub>为目标状态。</li>
<li>F：操作算子集，由所有可能的操作算子组成。</li>
<li>G：约束条件，由所有可能的约束条件组成。</li>
</ul>
<p>求解路径为S<sub>0</sub>到S<sub>g</sub>的路径，而状态空间的一个解为一个有限的状态序列。</p>
<h3 id="盲目搜索"><a href="#盲目搜索" class="headerlink" title="盲目搜索"></a>盲目搜索</h3><p>按照一定简单的顺序进行搜索。</p>
<h4 id="回溯策略"><a href="#回溯策略" class="headerlink" title="回溯策略"></a>回溯策略</h4><p>回溯是一种常用的搜索策略，它的基本思想是从一组可能的解中，通过逐步尝试并撤销（回溯）操作，寻找问题的解。<strong>在回溯过程中，如果当前的解不能满足约束条件，那么就撤销上一步的操作，回到上一步的状态，然后尝试其他的解</strong>。这个过程会一直持续，直到找到满足约束条件的解或者所有的解都已经尝试过。</p>
<ul>
<li><strong>PS表</strong>：保存当前搜索路径上的状态。</li>
<li><strong>NPS表</strong>：新的路径状态表，保存等待搜索的状态。</li>
<li><strong>NSS表</strong>：不可解状态表，保存非解的状态。</li>
</ul>
<p>&#x2F;&#x2F;&#x2F;</p>
<ul>
<li><strong>open表（NPS表）</strong>：存放待扩展的结点。</li>
<li><strong>close表（PS表和NSS表的合并）</strong>：存放已扩展的结点。</li>
</ul>
<h4 id="广度优先搜索（BFS，宽搜）"><a href="#广度优先搜索（BFS，宽搜）" class="headerlink" title="广度优先搜索（BFS，宽搜）"></a>广度优先搜索（BFS，宽搜）</h4><p>广度优先搜索（BFS，宽搜）是一种用于图和树的盲目搜索算法。</p>
<p>对于树而言，BFS按照宽度或者代价顺序，依次访问每一层的节点；对于图而言，BFS会依次访问离PS表中存放的节点中最近的节点。</p>
<h4 id="深度优先搜索（DFS，深搜）"><a href="#深度优先搜索（DFS，深搜）" class="headerlink" title="深度优先搜索（DFS，深搜）"></a>深度优先搜索（DFS，深搜）</h4><p>同样的，DFS也是一种用于图和树的盲目搜索算法。</p>
<p>DFS从根节点开始，<strong>沿着某一条路径尽可能深入地访问</strong>，直到无法继续为止，然后<strong>回溯</strong>到前一个节点，再选择另一条路径进行搜索，直到所有的节点都被访问过。</p>
<h3 id="启发式搜索"><a href="#启发式搜索" class="headerlink" title="启发式搜索"></a>启发式搜索</h3><p>在某些问题中由于其可能<strong>没有精确解</strong>或者<strong>状态空间巨大</strong>，可以考虑根据问题的特点，利用启发信息来指导搜索方向，以期望更快地找到目标状态。</p>
<p>在盲目搜索中，我们按照一定的顺序在open表中寻找下一个要扩展的结点，而在启发式搜索中，我们结合结点的启发信息，<strong>更改open表中原有的搜索顺序</strong>。根据以下公式，优先搜索$f(n)$最小的节点。</p>
<p>$$f(n) &#x3D; g(n) + h(n)$$</p>
<p>其中，</p>
<ul>
<li>$f(n)$：是<strong>代价函数</strong>，该值用来确定节点被选择的顺序。</li>
<li>$g(n)$：是<strong>实际代价函数</strong>，表示从初始状态到当前状态n的实际代价。</li>
<li>$h(n)$：是<strong>估价函数</strong>，用于评估从当前状态n到目标状态的预计代价。</li>
</ul>
<p><em>注意：启发式搜索并不能确保，且很大概率不能找到最优解，但是可以大大提高搜索效率。</em></p>
<h4 id="A-搜索和-A-算法"><a href="#A-搜索和-A-算法" class="headerlink" title="A 搜索和 A* 算法"></a>A 搜索和 A* 算法</h4><p>A搜索是最简单的一种启发式图搜索算法，它在BFS的基础上，<strong>加入了估价函数</strong>，每次选择$f(n)$最小的节点进行扩展。</p>
<p>设$h^{<em>}(n)$为从n到目标状态的最小代价，$h(n)$为从n到目标状态的估计代价，则有：<br>$$h^{</em>}(n)≥h(n)$$<br>受到满足此条件的某种限制下的A搜索算法称为A*算法（最佳图搜索算法）。</p>
<p>如果某一问题有解，那么利用A*算法可以找到最优解。</p>
<p>A* 搜索拥有：</p>
<ul>
<li>可容性</li>
<li>一致性</li>
</ul>
<h3 id="对抗搜索"><a href="#对抗搜索" class="headerlink" title="对抗搜索"></a>对抗搜索</h3><p>也称为<strong>博弈搜索</strong>，是一种特殊的搜索算法，用于解决博弈问题。在一个竞争的环境中，智能体之间通过竞争实现相反的利益，一方最大化这个利益，一方最小化这个利益。</p>
<p>在这里，我们主要讨论确定的完全、完美信息的两人零和动态博弈问题。</p>
<p><em>注意：完全信息博弈、完美信息博弈、零和博弈、动态博弈的概念将在博弈论笔记中解释</em></p>
<h4 id="最大最小搜索"><a href="#最大最小搜索" class="headerlink" title="最大最小搜索"></a>最大最小搜索</h4><p>最大最小搜索是一种最简单的对抗搜索算法。给定一个游戏搜索树，最大最小搜索算法会从叶节点开始，<strong>交替地选择最大值和最小值</strong>，直到搜索树的根节点。</p>
<p><em>相关概念：动态博弈的纳什均衡</em></p>
<p><strong>优点：</strong></p>
<ul>
<li>简单，容易实现。</li>
<li><strong>可以找到最优解</strong>。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>搜索空间巨大时，<strong>计算量巨大</strong>。</li>
<li>只能用于完全信息、完美信息、零和博弈。</li>
</ul>
<h4 id="Alpha-Beta-剪枝"><a href="#Alpha-Beta-剪枝" class="headerlink" title="Alpha-Beta 剪枝"></a>Alpha-Beta 剪枝</h4><p>Alpha-Beta 剪枝基于最大最小搜索进行优化，它可以<strong>减少搜索空间</strong>，从而提高搜索效率。</p>
<p>Alpha-Beta 剪枝的基本思想是，<strong>在搜索过程中，如果发现某个节点的值不会影响最终的结果，那么就不再搜索这个节点的子节点</strong>。</p>
<ul>
<li>alpha为玩家MAX目前得到的最高收益</li>
<li>beta为玩家MIN给对手的最低收益</li>
</ul>
<p>Alpha-Beta剪枝的过程如下：</p>
<ol>
<li>初始化alpha和beta值。<strong>alpha</strong>是当前节点的所有子节点中的<strong>最大值</strong>，<strong>beta</strong>是当前节点的所有子节点中的<strong>最小值</strong>。</li>
<li>对当前节点的每一个子节点进行<strong>深度优先搜索</strong>。在搜索过程中，如果发现某个子节点的值大于或等于beta（在最大层）或者小于或等于alpha（在最小层），那么就停止搜索这个子节点的其他子节点。</li>
<li>更新alpha或beta值。在最大层，如果某个子节点的值大于alpha，那么就更新alpha的值。在最小层，如果某个子节点的值小于beta，那么就更新beta的值。</li>
<li>返回alpha或beta值作为当前节点的值。</li>
</ol>
<figure class="highlight plaintext"><figcaption><span>code</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">function ALPHA-BETA-SEARCH(节点, alpha, beta)</span><br><span class="line">    alpha := -∞, beta := +∞;</span><br><span class="line">    if 节点是叶子节点 then:</span><br><span class="line">        return 节点的启发式值;</span><br><span class="line">    if 节点是最大化节点 then:</span><br><span class="line">        for 节点的每一个子节点</span><br><span class="line">            alpha := max(alpha, ALPHA-BETA-SEARCH(子节点, alpha, beta))</span><br><span class="line">            如果 beta &lt;= alpha</span><br><span class="line">                return; //(Beta剪枝)</span><br><span class="line">        return alpha;</span><br><span class="line">    else 节点是最小化节点:</span><br><span class="line">        for 节点的每一个子节点</span><br><span class="line">            beta := min(beta, ALPHA-BETA-SEARCH(子节点, alpha, beta))</span><br><span class="line">            if beta &lt;= alpha</span><br><span class="line">                return; //(Alpha剪枝)</span><br><span class="line">        return beta;</span><br></pre></td></tr></table></figure>

<h4 id="蒙特卡洛树搜索"><a href="#蒙特卡洛树搜索" class="headerlink" title="蒙特卡洛树搜索"></a>蒙特卡洛树搜索</h4><p>蒙特卡洛树搜索（Monte Carlo Tree Search，MCTS）是一种用于解决博弈问题的搜索算法。它的基本思想是通过<strong>随机模拟</strong>的方式，对<strong>搜索空间进行采样，然后根据采样结果来指导搜索过程</strong>。</p>
<p>蒙特卡洛树搜索的过程如下：</p>
<ol>
<li><p><strong>选择</strong>：从根节点开始，选择一个<strong>上限置信区间</strong>（Upper Confidence Bound for Trees，UCT）最高的节点。</p>
<ul>
<li>$UCB &#x3D; \bar{X_j} + C \sqrt{\frac{2 \ln{n}}{n_j}}$</li>
</ul>
<p> 其中：</p>
<ul>
<li>$\bar{X_j}$ 是当前节点的奖励值期望。</li>
<li>$C$ 是一个常数，用于控制探索和利用的平衡。一般$C&#x3D;2$。</li>
<li>$n$ 是当前节点的访问次数。</li>
<li>$n_j$ 是子节点的访问次数。</li>
</ul>
<p> $\sqrt{\frac{2 \ln{n}}{n_j}}$可以表示在这个节点下探索的程度。为了保证探索的广度，子节点访问得越多，UCB值会越小。</p>
</li>
<li><p><strong>扩展</strong>：如果当前选择的节点不是叶子节点，那么创建一个或多个新的子节点。新的子节点代表了从当前状态出发可以采取的行动。</p>
</li>
<li><p><strong>模拟&#x2F;搜索</strong>：从当前节点开始，进行一次随机模拟。模拟就是按照某种策略（例如随机策略&#x2F;AlphaGo使用神经网络）进行游戏，直到游戏结束或达到停止条件；同时也可以进行对子节点的搜索，重复步骤1、2。</p>
</li>
<li><p><strong>回溯</strong>：根据模拟的结果，更新当前节点和它的所有祖先节点。更新的内容包括节点的<strong>访问次数</strong>和<strong>胜率</strong>。</p>
</li>
</ol>
<h2 id="2-智能优化方法"><a href="#2-智能优化方法" class="headerlink" title="2. 智能优化方法"></a>2. 智能优化方法</h2><p>智能计算是一类基于生物进化过程的启发式优化算法。人们根据生物进化过程中的各种行为和功能进行模拟，根据其原理来求解问题。</p>
<p>智能优化方法通常包括<strong>进化计算</strong>和<strong>群智能算法</strong>。</p>
<h3 id="进化计算"><a href="#进化计算" class="headerlink" title="进化计算"></a>进化计算</h3><p>进化算法（Evolutionary Algorithm, EA）是基于自然选择和自然遗传等生物进化机制的一种<strong>搜索算法</strong>，源于达尔文进化论中的“适者生存”。</p>
<p>进化算法是一个算法簇，包含了<strong>遗传算法</strong>、进化策略等等，各种算法变体很相近，思路也类似。</p>
<p>进化算法的基本思想是，通过对候选解进行<strong>变异</strong>、<strong>重组</strong>和<strong>选择</strong>等操作，逐步地<strong>优化</strong>候选解的质量，从而找到问题的最优解。</p>
<p>进化算法的设计原则：</p>
<ul>
<li>适用性</li>
<li>可靠性</li>
<li>收敛性</li>
<li>稳定性</li>
<li>生物类比原则</li>
</ul>
<h4 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h4><p>遗传算法（Genetic Algorithm, GA）是一种最基本的进化算法的一种。其建立的种群中<strong>每一个个体都是解空间上的一个可行解</strong>，通过模拟生物进化过程，从而找到解空间中的最优解。</p>
<p>遗传算法的主要特点有：</p>
<ul>
<li><strong>广泛适用性</strong>：遗传算法对优化问题的数学要求较少，无论问题是线性的还是非线性的，离散的还是连续的，都可以处理。</li>
<li><strong>高效搜索</strong>：遗传算法利用随机技术，对一个被编码的参数空间进行高效率搜索。</li>
<li><strong>群体搜索策略</strong>：遗传算法采用群体搜索策略，易于并行化。</li>
<li><strong>基于适应度函数</strong>：遗传算法仅用适应度函数值来评估个体，并在此基础上进行遗传操作，使种群中个体之间进行信息交换。</li>
<li><strong>全局搜索能力</strong>：进化算子的各态历经性使得遗传算法能够非常有效地进行概率意义的全局搜索。</li>
<li><strong>灵活性</strong>：遗传算法对于各种特殊问题可以提供极大的灵活性来混合构造领域独立的启发式，从而保证算法的有效性。</li>
</ul>
<p>以下是概念对应表：</p>
<table>
<thead>
<tr>
<th align="center">生物遗传概念</th>
<th align="center">遗传算法中的概念</th>
</tr>
</thead>
<tbody><tr>
<td align="center">适者生存</td>
<td align="center">适应度函数值高的解存活的可能性大</td>
</tr>
<tr>
<td align="center">个体</td>
<td align="center">一个解</td>
</tr>
<tr>
<td align="center">染色体</td>
<td align="center">解的对应编码</td>
</tr>
<tr>
<td align="center">基因</td>
<td align="center">解的一个部分</td>
</tr>
<tr>
<td align="center">种群</td>
<td align="center">解的集合</td>
</tr>
<tr>
<td align="center">适应性</td>
<td align="center">解的质量（目标函数&#x2F;适应度函数值）</td>
</tr>
<tr>
<td align="center">选择</td>
<td align="center">选择优良的解进行繁殖</td>
</tr>
<tr>
<td align="center">交叉</td>
<td align="center">选择的两个染色体融合成新的染色体</td>
</tr>
<tr>
<td align="center">变异</td>
<td align="center">编码的某一分量发生变化</td>
</tr>
</tbody></table>
<hr>
<p><strong>遗传算法的一般步骤为：</strong></p>
<h5 id="1-编码"><a href="#1-编码" class="headerlink" title="1. 编码"></a>1. 编码</h5><p>编码的数学理论基础是模式定理，这里不做展开。</p>
<p>遗传算法的编码方式有很多种，常用的编码方式有：</p>
<h6 id="位串编码"><a href="#位串编码" class="headerlink" title="位串编码"></a>位串编码</h6><p>将问题空间的参数编码为一维排列的染色体的方法</p>
<ol>
<li><p><strong>二进制编码</strong>：用若干二进制数表示一个个体，将原问题的解空间映射到空间$B&#x3D;{0,1}$上。</p>
<ul>
<li>优点：<ul>
<li>解释性强</li>
<li><strong>容易实现交叉、变异等操作</strong></li>
<li>可处理的模式数最多</li>
</ul>
</li>
<li>缺点：<ul>
<li>整数域映射到二进制数域后，某些相邻整数的海明距离大，遗传算子的搜索效率变低</li>
<li>有更长的染色体</li>
<li>需要预先给出解的精度</li>
</ul>
</li>
</ul>
</li>
<li><p>Gray编码：对二进制编码进行变换。</p>
</li>
<li><p>n进制编码：将原问题的解空间映射到空间${0,1,…,n-1}$上。</p>
<p> <em>由模式定理可推导出不同进制下的对于算法性能的影响。</em></p>
</li>
</ol>
<h6 id="实数编码"><a href="#实数编码" class="headerlink" title="实数编码"></a>实数编码</h6><p>直接使用浮点数表示解的染色体的方法。</p>
<h6 id="多参数级联编码"><a href="#多参数级联编码" class="headerlink" title="多参数级联编码"></a>多参数级联编码</h6><p>将多个二进制编码下的字串连成完整的染色体，每个子串可以有不同的长度和参数范围。</p>
<h5 id="2-种群设定"><a href="#2-种群设定" class="headerlink" title="2. 种群设定"></a>2. 种群设定</h5><p>在掌握解的分布下，产生一定数目的个体，作为初始种群。</p>
<p>若种群规模太小，算法性能差，易陷入局部最优；<br>若种群规模太大，算法复杂度高。</p>
<p><em>由模式定理，群体规模为$M$时，遗传操作课生成和检测$M^3$个模式。</em></p>
<h5 id="3-设定适应度函数"><a href="#3-设定适应度函数" class="headerlink" title="3. 设定适应度函数"></a>3. 设定适应度函数</h5><p>适应度函数是遗传算法的核心，它是对解的质量的度量。与目标函数不同的是，<strong>适应度越高，解的质量越好</strong>。同时，我们需要保证适应度函数的值域<strong>非负</strong>。</p>
<h6 id="尺度变换（变比）"><a href="#尺度变换（变比）" class="headerlink" title="尺度变换（变比）"></a>尺度变换（变比）</h6><p>在很多情况下，简单的适应度函数对于遗传算法来说性态较差，容易出现<strong>超级个体</strong>或群体内<strong>竞争不足</strong>的情况。需要对其进行尺度变换，以确保算法在搜索时能保证良好的性能。</p>
<ol>
<li>线性变换<br>$$f’(x) &#x3D; a f(x) + b$$</li>
<li>幂函数变换<br>$$f’(x) &#x3D; f(x)^K$$</li>
<li>指数变换<br>$$f’(x) &#x3D; e^{-af(x)}$$</li>
<li>Sigma变比：<br>$$f’(x) &#x3D; f(x) - (\mu - c * \sigma)$$</li>
</ol>
<h5 id="4-选择"><a href="#4-选择" class="headerlink" title="4. 选择"></a>4. 选择</h5><p>从当前群体按照一定概率选择出优良个体繁衍下一代，<strong>个体适应度越高，被选择的机会越多。</strong></p>
<ul>
<li>线性排序：将群体中的个体按照适应度函数值从大到小排序，然后线性地分配概率。</li>
<li>非线性排序：将群体中的个体按照适应度函数值从大到小排序，然后指数地分配概率。</li>
</ul>
<h6 id="轮盘赌选择"><a href="#轮盘赌选择" class="headerlink" title="轮盘赌选择"></a>轮盘赌选择</h6><p>将群体中的个体按照适应度函数值从大到小排序，然后将适应度函数值最大的个体放在轮盘的最上方，适应度函数值最小的个体放在轮盘的最下方，然后将轮盘分成若干个扇形，每个扇形的大小与个体的适应度函数值成正比。</p>
<h6 id="锦标赛选择"><a href="#锦标赛选择" class="headerlink" title="锦标赛选择"></a>锦标赛选择</h6><ul>
<li><p>在群体中反复随机选择多个个体，然后从中选择适应度函数值最大的个体。</p>
</li>
<li><p>反复按轮盘赌选择两个个体，取适应度高的个体作为父代。（锦标赛选择的一个变体，随机竞争方法）</p>
</li>
</ul>
<h6 id="保留策略"><a href="#保留策略" class="headerlink" title="保留策略"></a>保留策略</h6><p>把群体中适应度最高的个体直接保留到下一代。</p>
<h5 id="5-交叉"><a href="#5-交叉" class="headerlink" title="5. 交叉"></a>5. 交叉</h5><p>通过<strong>交换染色体中的基因</strong>，产生新的个体。</p>
<h6 id="单点交叉"><a href="#单点交叉" class="headerlink" title="单点交叉"></a>单点交叉</h6><p>在染色体上随机选择一个交叉点，将两个染色体在交叉点处的基因进行交换。</p>
<h6 id="多点交叉"><a href="#多点交叉" class="headerlink" title="多点交叉"></a>多点交叉</h6><p>在染色体上随机选择多个交叉点，将两个染色体在交叉点处的基因进行交换。</p>
<p><em>1. 模式定理可以解释n点交叉对算法性能的影响。</em></p>
<p><em>2. 对染色体中相同的部分进行交叉是非法的，对于这种情况，可以使用部分匹配交叉进行调整。</em></p>
<h5 id="6-变异"><a href="#6-变异" class="headerlink" title="6. 变异"></a>6. 变异</h5><p>通过<strong>改变染色体中的基因</strong>，产生新的个体。</p>
<ul>
<li><strong>位点变异</strong>：以一定概率直接修改基因值。</li>
<li><strong>逆转变异</strong>：随机选择染色体上的两个位置，将这两个位置之间的基因逆序。</li>
<li><strong>插入变异</strong>：在染色体上随机选择一个码位，将这个码位上的基因插入到染色体上的另一个随机位置。</li>
<li><strong>交换变异</strong>：在染色体上随机选择两个码位，将这两个码位上的基因进行交换。</li>
<li><strong>移动变异</strong>：在染色体上随机选取一个基因，移动随机位数。</li>
<li><strong>自适应变异</strong>：类似于位点变异，但概率根据个体的多样性进行自适应调整。</li>
</ul>
<p><strong>在计算适应值、选择、交叉、变异后，得到新的种群，然后重复以上步骤，直到满足终止条件。</strong></p>
<h3 id="群智能优化"><a href="#群智能优化" class="headerlink" title="群智能优化"></a>群智能优化</h3><p>群智能算法（Swarm Algorithms, SI）是一类基于群体智能的启发式优化算法，源于生物群体中的群体智能行为。</p>
<p>与进化计算相同，群智能算法也是<strong>受自然现象启发、基于种群且存在相互作用的启发式随机搜索方法</strong>，而不同之处在于，群智能算法更注重<strong>个体之间的相互协同作用</strong>。</p>
<h4 id="粒子群优化"><a href="#粒子群优化" class="headerlink" title="粒子群优化"></a>粒子群优化</h4><p>粒子群优化（Particle Swarm Optimization，PSO）是一种基于群体智能的优化算法。它模拟的是鸟群觅食的行为。粒子群优化算法的基本思想是：在搜索空间中，有一群粒子在移动，每个粒子代表一个可能的解。每个粒子都有一个<strong>速度</strong>，决定了它的<strong>移动方向</strong>和<strong>移动快慢</strong>，同时每个粒子都有一个<strong>记忆</strong>，记住了它找到的最好的位置。在每一步，<strong>粒子根据自己的记忆（个体极值）和群体的最好位置（全局极值）来更新自己的速度和位置</strong>。</p>
<p><strong>基本的PSO算法的速度和位置更新公式为：</strong><br>$$v_i^{k+1} &#x3D; w v_i^k + c_1 r_1 (p_i^k - x_i^k) + c_2 r_2 (p_g^k - x_i^k)$$<br>$$x_i^{k+1} &#x3D; x_i^k + v_i^{k+1}$$<br>其中，</p>
<ul>
<li>$w$：<strong>惯性权重因子</strong>，通常为1.0，或可以进行模糊调节。</li>
<li>$c_1, c_2$：<strong>非负的加速因子</strong>，通常为2.0，每一维度变化10%-20%。</li>
<li>$r_1, r_2$：服从$[0, a_{1}], [0, a_{2}]$均匀分布的<strong>随机数</strong>。</li>
<li>$p_i^k, p_g^k$：粒子$i$在第$k$代的个体极值，和粒子$i$在第$k$代的全局极值。</li>
</ul>
<p>&#x2F;&#x2F;&#x2F;</p>
<ul>
<li>$c_1 r_1 (p_i^k - x_i^k)$：个体认知分量，表示粒子$i$在第$k$代的个体极值与当前位置的差距。</li>
<li>$c_2 r_2 (p_g^k - x_i^k)$：群体社会分量，表示粒子间的信息共享与相互合作。</li>
</ul>
<p><strong>公式的参数分析：</strong></p>
<ul>
<li>若$c_1 &gt; 0, c_2 &gt; 0$，则称该算法为<strong>PSO全模型</strong>。</li>
<li>若$c_1 &#x3D; 0, c_2 &gt; 0$，则称该算法为<strong>PSO社会模型</strong>，粒子没有认知能力，只有个体间的相互作用，容易陷入局部最优。</li>
<li>若$c_1 &gt; 0, c_2 &#x3D; 0$，则称该算法为<strong>PSO认知模型</strong>，粒子间没有共享信息，找到最优解的概率非常小。</li>
<li>若$c_1 &#x3D; 0, c_2 &#x3D; 0$，则称该算法为<strong>PSO无私模型</strong>，例子将已知以当前的速度飞行，直到到达边界。</li>
<li>若$w &#x3D; 0$，则速度只取决于当前位置和历史最好位置，速度本身<strong>没有记忆性</strong>。</li>
</ul>
<p><strong>其它超参数：</strong></p>
<ul>
<li>$V_{max}$：粒子的最大速度，防止粒子过快地飞过搜索空间，从而错过可能的最优解。</li>
</ul>
<p><strong>粗糙集理论</strong></p>
<p>太难了，看不懂，忽略。</p>
<p><em>可以基于粗糙集理论对粒子群和遗传算法进行混合。具体来说，就是对于适应度高的粒子使用PSO的方法更新速度和位置，适应度低的使用GA的交叉和变异进行更新，有利于减小表现差的粒子对群体的负面影响。</em></p>
<p><em>另外，还可以使用PSO进行特征提取。</em></p>
<h4 id="蚁群算法"><a href="#蚁群算法" class="headerlink" title="蚁群算法"></a>蚁群算法</h4><p>蚁群算法（Ant Colony Optimization，ACO）是一种基于群体智能的优化算法。它模拟的是蚂蚁在寻找食物的过程。蚂蚁在寻找食物的过程中，会释放一种<strong>信息素</strong>，这种信息素会随着时间的推移而逐渐蒸发。当蚂蚁在寻找食物的过程中，<strong>会优先选择信息素浓度高的路径，从而使得信息素浓度更高的路径被更多的蚂蚁选择，形成正反馈</strong>，最终所有的蚂蚁都会选择信息素浓度最高的路径。</p>
<p><strong>蚁群系统模型的公式为：</strong><br>$$P_{ij}^k &#x3D; \frac{[\tau_{ij}(k)]^\alpha [\eta_{ij}]^\beta}{\sum_{l \in N_i^k} [\tau_{il}(k)]^\alpha [\eta_{il}]^\beta}$$<br>$$\tau_{ij}(k+1) &#x3D; \rho \tau_{ij}(k) + \sum_{k&#x3D;1}^m \Delta \tau_{ij}^k$$<br>其中，</p>
<ul>
<li>$P_{ij}^k$：蚂蚁$k$在节点$i$选择节点$j$的概率。</li>
<li>$\tau_{ij}(k)$：节点$i$到节点$j$的信息素浓度。</li>
<li>$\eta_{ij}$：节点$i$到节点$j$的启发式信息。</li>
<li>$\alpha$：信息素重要程度因子。</li>
<li>$\beta$：启发式信息重要程度因子。</li>
</ul>
<p>&#x2F;&#x2F;&#x2F;</p>
<ul>
<li>$1 - \rho$：信息素挥发因子，$0 \leq \rho \leq 1$。</li>
<li>$\sum_{k&#x3D;1}^m \Delta \tau_{ij}^k &#x3D; \Delta \tau_{ij}^k$：蚂蚁$k$在节点$i$到节点$j$的信息素增量。</li>
</ul>
<p><strong>公式的参数分析：</strong></p>
<ul>
<li>$\alpha$越大，蚂蚁越倾向于选择其它蚂蚁走过的路径，转移概率越接近于贪婪规则，易于陷入局部最优。</li>
<li>$\alpha &#x3D; 0$时，不存在信息素，算法成为多重起点的随机贪婪算法。</li>
<li>$\beta$越大，蚂蚁越倾向于选择启发式信息最大的路径，转移概率越接近于启发式规则，也易于陷入局部最优。</li>
<li>$\beta &#x3D; 0$时，蚂蚁只根据信息素选择路径，算法成为纯粹的正反馈启发式算法，不考虑局部启发信息。</li>
</ul>
<p>&#x2F;&#x2F;&#x2F;</p>
<ul>
<li>$1 - \rho$越大，信息素挥发越快，记忆减弱，探索过的路径更可能被遗忘，从而可能导致反复搜索，算法性能降低。</li>
<li>$1 - \rho$越小，信息素挥发越慢，早期过多较差的路径可能被记得较牢，收敛速度降低。</li>
</ul>
<h5 id="蚂蚁圈系统"><a href="#蚂蚁圈系统" class="headerlink" title="蚂蚁圈系统"></a>蚂蚁圈系统</h5><p>蚂蚁圈系统（Ant Colony System，ACS）中单只蚂蚁所访问路径上的信息素浓度更新规则为：<br>$$\Delta \tau_{ij}^k(t) &#x3D; \frac{Q}{L_k(t)}$$<br>其中，</p>
<ul>
<li>Q：信息素增量常数</li>
<li>$L_k(t)$：目标函数，蚂蚁$k$在第$t$次迭代中的总路径长度</li>
<li>$\frac{Q}{L_k(t)}$：<strong>全局信息</strong>，即蚂蚁完成一个循环后，更新所有路径上的信息</li>
</ul>
<h5 id="蚂蚁数量系统"><a href="#蚂蚁数量系统" class="headerlink" title="蚂蚁数量系统"></a>蚂蚁数量系统</h5><p>蚂蚁数量系统（Ant Quantity System，AQS）中单只蚂蚁所访问路径上的信息素浓度更新规则为：<br>$$\Delta \tau_{ij}^k(t) &#x3D; \frac{Q}{d_k}$$<br>其中，</p>
<ul>
<li>$d_k$：目标函数，蚂蚁$k$一步行走的路径长度</li>
<li>$\frac{Q}{d_k}$：<strong>局部信息</strong>，即蚂蚁完成一步行走后，更新当前路径上的信息</li>
</ul>
<h5 id="蚂蚁密度系统"><a href="#蚂蚁密度系统" class="headerlink" title="蚂蚁密度系统"></a>蚂蚁密度系统</h5><p>蚂蚁密度系统（Ant Density System，ADS）中单只蚂蚁所访问路径上的信息素浓度更新规则为：<br>$$\Delta \tau_{ij}^k(t) &#x3D; Q$$<br>其中，</p>
<ul>
<li>Q：<strong>局部信息</strong>，即蚂蚁每走一步更新的残留信息素浓度。</li>
</ul>
<hr>
<p><strong>蚁群算法的一般步骤为：</strong></p>
<ol>
<li>初始化信息素浓度和蚂蚁的个数和位置。</li>
<li>重复以下步骤，直到满足终止条件：<ol>
<li>每只蚂蚁根据信息素浓度和启发式信息选择下一步要走的位置。</li>
<li>每只蚂蚁走一步，更新信息素浓度。</li>
</ol>
</li>
</ol>
<h2 id="3-机器学习"><a href="#3-机器学习" class="headerlink" title="3. 机器学习"></a>3. 机器学习</h2><p>机器学习（Machine Learning，ML）是智能体从数据中自动学习知识的一种人工智能方法。</p>
<p>这是一种新的编程范式。传统的面向过程的编程范式为：<br>$$问题 \xrightarrow{编写规则} 结果$$<br>而机器学习的编程范式为：<br>$$数据 \xrightarrow{结果} 规则$$</p>
<h3 id="机器学习的基本概念"><a href="#机器学习的基本概念" class="headerlink" title="机器学习的基本概念"></a>机器学习的基本概念</h3><h4 id="机器学习的分类"><a href="#机器学习的分类" class="headerlink" title="机器学习的分类"></a>机器学习的分类</h4><p>从数据角度出发，可以分为：</p>
<ul>
<li><strong>监督学习</strong>：给定一组<strong>有标签</strong>的训练数据，通过学习训练数据，得到模型。</li>
<li><strong>无监督学习</strong>：给定一组<strong>无标签</strong>的训练数据，通过学习训练数据，得到模型。</li>
<li><strong>强化学习</strong>：给定一个<strong>环境</strong>，智能体通过<strong>与环境的交互</strong>，得到奖励，从而学习到最优策略。</li>
</ul>
<p>从任务角度出发，可以分为：</p>
<ul>
<li><strong>分类</strong>：将数据划分为若干类别（离散）。</li>
<li><strong>回归</strong>：预测数据的数值（连续）。</li>
</ul>
<h4 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h4><h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>损失函数（Loss Function）用于衡量模型的预测值与真实值之间的差距。</p>
<p>损失函数的种类有很多，常用的损失函数有：</p>
<ul>
<li><strong>均方差</strong>（Mean Squared Error，MSE）：用于回归问题。<br>$$MSE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n (y_i - \hat{y_i})^2$$</li>
<li><strong>交叉熵</strong>（Cross Entropy，CE）：用于分类问题。<br>$$CE &#x3D; - \sum_{i&#x3D;1}^n y_i \log{\hat{y_i}}$$</li>
<li><strong>对数损失&#x2F;二元交叉熵</strong>（Binary Cross Entropy，BCE）：用于二分类问题。<br>$$BCE &#x3D; - \sum_{i&#x3D;1}^n y_i \log{\hat{y_i}} + (1 - y_i) \log{(1 - \hat{y_i})}$$</li>
</ul>
<h5 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h5><p>模型评估（Model Evaluation）用于衡量模型的性能。</p>
<ul>
<li>经验风险（Empirical Risk）：模型在训练集上的损失。</li>
<li>期望风险（Expected Risk）：模型在测试集上的损失。</li>
</ul>
<h5 id="过学习与欠学习（过拟合-欠拟合）"><a href="#过学习与欠学习（过拟合-欠拟合）" class="headerlink" title="过学习与欠学习（过拟合&#x2F;欠拟合）"></a>过学习与欠学习（过拟合&#x2F;欠拟合）</h5><ul>
<li><strong>过拟合</strong>（Overfitting）：模型在训练集上的损失远小于模型在测试集上的损失。</li>
<li><strong>欠拟合</strong>（Underfitting）：模型在训练集和测试集上表现都不好。</li>
</ul>
<p>常用的解决过拟合的方法有：<strong>降低模型复杂度</strong>、<strong>添加正则化项或惩罚项</strong>、<strong>数据增强</strong>、<strong>早停</strong>、<strong>添加Dropout层</strong>等。</p>
<p><em>这部分将在其它文章中详解。</em></p>
<h5 id="判别模型和生成模型"><a href="#判别模型和生成模型" class="headerlink" title="判别模型和生成模型"></a>判别模型和生成模型</h5><ul>
<li><strong>判别模型</strong>（Discriminative Model）：直接对条件概率$P(y|x)$建模，例如<strong>逻辑回归</strong>、<strong>SVM</strong>、<strong>决策树</strong>、<strong>神经网络</strong>、<strong>Ada boosting</strong>等。</li>
<li><strong>生成模型</strong>（Generative Model）：对联合概率$P(x, y)$建模，然后通过贝叶斯公式计算条件概率$P(y|x)$，例如<strong>朴素贝叶斯</strong>、<strong>隐马尔可夫模型</strong>、<strong>高斯混合模型</strong>等。</li>
</ul>
<h5 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h5><p>线性回归（Linear Regression）是一种用于回归问题的监督学习算法。它的基本思想是，<strong>假设数据的特征和标签之间存在线性关系</strong>，然后使用<strong>最小二乘法（损失函数为均方差）</strong>，从而得到最优的模型参数。</p>
<p>线性回归的公式为：<br>$$\hat{y} &#x3D; w^T x + b$$<br>其中，</p>
<ul>
<li>$x$：数据的特征。</li>
<li>$y$：数据的标签。</li>
<li>$w$：模型的权重。</li>
<li>$b$：模型的偏置。</li>
</ul>
<h5 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h5><p>逻辑回归（Logistic Regression）是一种用于<strong>分类</strong>问题的监督学习算法。它额外引入了Sigmoid函数，将拟合的结果转换为概率，用于分类，可以解决非线性问题。</p>
<h5 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h5><p>支持向量机（Support Vector Machine，SVM）是一种用于<strong>分类</strong>和<strong>回归</strong>问题的监督学习算法。它的基本思想是，<strong>将数据映射到高维空间，然后在高维空间中找到一个最优的超平面，从而实现分类</strong>。</p>
<p>只有对线性可分的数据，才可以使用SVM。对于线性不可分的数据，可以使用<strong>核函数</strong>将数据<strong>映射到高维空间</strong>，然后再使用SVM。</p>
<p>核函数的种类有很多，常用的核函数有：</p>
<ul>
<li><strong>线性核</strong>：$K(x, y) &#x3D; x^T y$，默认的核函数。</li>
<li><strong>多项式核</strong>：$K(x, y) &#x3D; (x^T y + c)^d$，得到q阶多项式分类器。</li>
<li><strong>高斯核</strong>：$K(x, y) &#x3D; \exp{(- \frac{|x - y|^2}{2 \sigma^2})}$，每个基函数中心对应一个支持向量，权值由算法自动决定。</li>
<li><strong>Sigmoid核</strong>：$K(x, y) &#x3D; \tanh{(\alpha x^T y + c)}$，包含一个隐层的多层感知机，隐层节点数由算法自动决定。</li>
</ul>
<h5 id="K-最近邻"><a href="#K-最近邻" class="headerlink" title="K-最近邻"></a>K-最近邻</h5><p>K-最近邻（K-Nearest Neighbor，KNN）是一种用于<strong>分类</strong>和<strong>回归</strong>问题的监督学习算法。它的基本思想是，<strong>对于一个新的数据，找到与它最近的K个数据，然后根据这K个数据的标签，来预测新数据的标签</strong>。</p>
<p><em>关于数据集划分、正则化项、K值的选择、模型评价标准（混淆矩阵、ROC曲线等）、距离等问题，将在其它文章中详解。</em></p>
<h5 id="提升算法"><a href="#提升算法" class="headerlink" title="提升算法"></a>提升算法</h5><p><strong>集成学习</strong>（Ensemble Learning）是一种机器学习策略，它结合了多个模型的预测结果，以获得更好的性能。</p>
<p>集成学习的主要类型包括：</p>
<ul>
<li><strong>Bagging</strong>：例如<strong>随机森林</strong>（Random Forest），它通过创建多个不同的训练集，并在每个训练集上训练一个模型，然后将这些模型的预测结果进行平均或者投票。</li>
<li><strong>Boosting</strong>：例如<strong>梯度提升</strong>（Gradient Boosting），它通过顺序地训练模型，每个模型都试图纠正前一个模型的错误。</li>
<li><strong>Stacking</strong>：它通过训练一个元模型（meta-model），来学习如何最好地结合基模型的预测结果。</li>
</ul>
<p>提升算法（Boosting）是集成学习（Ensemble Learning）的一种，其的基本思想是对于一个复杂的分类任务，可以将其分解为若干子任务，然后将子任务的结果进行组合，从而得到最终的结果。</p>
<p>在子任务上训练的分类器为<strong>弱分类器</strong>（Weak Classifier），将弱分类器组合的分类器为<strong>强分类器</strong>（Strong Classifier）。</p>
<h6 id="Ada-Boosting"><a href="#Ada-Boosting" class="headerlink" title="Ada Boosting"></a>Ada Boosting</h6><p>Ada Boosting以<strong>线性组合</strong>的形式，将弱分类器组合成强分类器。</p>
<p>算法描述：</p>
<ol>
<li>初始化训练集的权值分布，$D_1 &#x3D; (w_{11}, …, w_{1i}, …, w_{1N})$，其中$w_{1i} &#x3D; \frac{1}{N}$。</li>
<li>对于$m &#x3D; 1, 2, …, M$：<ol>
<li>使用具有权值分布$D_m$的训练集训练出弱分类器$G_m(x)$。</li>
<li>计算$G_m(x)$在训练集上的误差率：<br> $$e_m &#x3D; \sum_{i&#x3D;1}^N P(G_m(x_i) \neq y_i) &#x3D; \sum_{i&#x3D;1}^N w_{mi} I(G_m(x_i) \neq y_i)$$</li>
<li>计算$G_m(x)$的系数：<br> $$\alpha_m &#x3D; \frac{1}{2} \log{\frac{1 - e_m}{e_m}}$$</li>
<li>更新训练集的权值分布：<br> $$D_{m+1} &#x3D; (w_{m+1, 1}, …, w_{m+1, i}, …, w_{m+1, N})$$<br> $$w_{m+1, i} &#x3D; \frac{w_{mi}}{Z_m} \exp{(- \alpha_m y_i G_m(x_i))}$$<br> $$Z_m &#x3D; \sum_{i&#x3D;1}^N w_{mi} \exp{(- \alpha_m y_i G_m(x_i))}$$</li>
</ol>
</li>
</ol>
<h4 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h4><h5 id="K-均值算法"><a href="#K-均值算法" class="headerlink" title="K-均值算法"></a>K-均值算法</h5><p>K-均值算法（K-Means）是一种用于<strong>聚类</strong>问题的无监督学习算法。它的基本思想是，<strong>将数据划分为K个簇，使得簇内的数据点尽可能接近，而簇间的数据点尽可能远离</strong>。</p>
<p>算法过程：</p>
<ol>
<li>随机选择K个数据点作为初始的聚类中心。</li>
<li>对于每个数据点，计算其与每个聚类中心的距离，将其划分到距离最近的聚类中心所在的簇。</li>
<li>对于每个簇，计算其所有数据点的均值，将其作为新的聚类中心。</li>
<li>重复步骤2和步骤3，直到聚类中心不再发生变化或达到迭代轮次上限。</li>
</ol>
<p>K-Means可以看作是<strong>最小化簇内平方和</strong>的过程。</p>
<p>K-Means的缺点是，<strong>需要预先指定簇的个数</strong>，而且<strong>对于不同的初始聚类中心，可能会得到不同的结果</strong>，其次<strong>算法的时间复杂度高</strong>。</p>
<h5 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h5><p>主成分分析（Principal Component Analysis，PCA）是一种用于<strong>降维</strong>的无监督学习算法。它的基本思想是，<strong>根据奥卡姆剃刀原理，将高维数据映射到低维空间，使得映射后的数据尽可能保留原始数据的信息</strong>。</p>
<p>PCA在降维时，需要尽可能保留原始数据的信息，因此需要选择<strong>方差最大的方向</strong>进行投影。</p>
<h2 id="4-深度学习"><a href="#4-深度学习" class="headerlink" title="4. 深度学习"></a>4. 深度学习</h2><p>深度学习（Deep Learning，DL）是机器学习的一种。在传统机器学习前，需要对数据进行复杂的<strong>数据预处理</strong>、<strong>特征提取</strong>和<strong>特征选择</strong>，而<strong>深度学习可以自动地从原始数据中学习到特征</strong>，从而避免了这些繁琐的工作。</p>
<p>深度学习的基本思想是，<strong>通过多层简单的非线性变换，将原始数据映射到更高层次、更加抽象的空间中，再进行学习</strong>。</p>
<p><em>深度学习的历史将在其它文章中详解。</em></p>
<h3 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h3><p>人工神经网络（Artificial Neural Network，ANN）是深度学习的基础，它是一种模拟人脑神经元网络的计算模型。</p>
<h4 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h4><p>感知机（Perceptron）是一种最简单的神经网络，<strong>它将多个输入加权求和，然后通过激活函数得到输出</strong>。</p>
<p>感知机的公式为：<br>$$y &#x3D; f(\sum_{i&#x3D;1}^n w_i x_i + b)$$<br>其中，</p>
<ul>
<li>$x_i$：输入。</li>
<li>$w_i$：权重。</li>
<li>$b$：偏置。</li>
<li>$f$：激活函数。</li>
</ul>
<h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p>激活函数（Activation Function）用于引入非线性因素，从而使得神经网络可以解决非线性问题。</p>
<p>常用的激活函数有：</p>
<ul>
<li><strong>Sigmoid函数</strong>：$f(x) &#x3D; \frac{1}{1 + e^{-x}}$，将输入映射到$(0, 1)$。</li>
<li><strong>Tanh函数</strong>：$f(x) &#x3D; \frac{e^x - e^{-x}}{e^x + e^{-x}}$，将输入映射到$(-1, 1)$。</li>
<li><strong>ReLU函数</strong>：$f(x) &#x3D; \max{(0, x)}$，将输入映射到$[0, +\infty)$。</li>
<li><strong>Leaky ReLU函数</strong>：$f(x) &#x3D; \max{(0.01x, x)}$，将输入映射到$[0.01x, +\infty)$。</li>
<li><strong>Softmax函数</strong>：$f(x_i) &#x3D; \frac{e^{x_i}}{\sum_{j&#x3D;1}^n e^{x_j}}$，将输入映射到$(0, 1)$，并且所有输出的和为1。</li>
</ul>
<p><em>关于激活函数的选择和具体作用，将在其它文章中详解。</em></p>
<h5 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h5><p>反向传播算法（Back Propagation，BP）是一种用于训练神经网络的算法，它的基本思想是，<strong>通过链式法则，将误差从输出层向输入层进行反向传播，然后根据误差对权重和偏置进行更新</strong>。</p>
<p>尽管现在几乎不再只使用BP算法，但是其在历史上的作用是里程碑式的。</p>
<p>BP算法的缺点为：</p>
<ul>
<li><strong>梯度消失：在反向传播的过程中，梯度会不断地缩小，从而导致梯度消失。</strong></li>
<li><strong>局部最优：BP算法只能保证收敛到局部最优解，而不能保证收敛到全局最优解。</strong></li>
<li><strong>计算量大</strong>：BP算法需要计算每个权重的梯度，计算量大。</li>
<li><strong>需要预先指定网络结构</strong>：BP算法需要预先指定网络结构，而且网络结构的选择对于最终的结果有很大的影响。</li>
</ul>
<h4 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h4><p>多层感知机（Multi-Layer Perceptron，MLP）是一种最简单的神经网络，它由<strong>输入层</strong>、<strong>隐藏层</strong>和<strong>输出层</strong>组成，其中隐藏层可以有多层。</p>
<h5 id="前向传播算法"><a href="#前向传播算法" class="headerlink" title="前向传播算法"></a>前向传播算法</h5><p>前向传播算法（Forward Propagation）用于计算神经网络的输出。</p>
<h5 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h5><p>优化器（Optimizer）用于更新神经网络的参数。</p>
<h6 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h6><p>随机梯度下降（Stochastic Gradient Descent，SGD）是一种最简单的优化器，它的基本思想是，<strong>在每一次迭代中，随机选择一个样本，然后计算该样本的梯度，最后反向传播更新参数</strong>。</p>
<p><em>其它优化器的介绍、选择、作用、优缺点将在其它文章中详解。</em></p>
<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><p>1958年，Hubel和Wiesel发现，动物的视觉皮层中的神经元对于图像信号输入时分级的。</p>
<p>卷积神经网络（Convolutional Neural Network，CNN）是一种用于<strong>图像</strong>和<strong>语音</strong>等数据的深度学习算法，它具有<strong>平移不变</strong>的特性。</p>
<p><strong>卷积层</strong></p>
<p>卷积层（Convolutional Layer）是卷积神经网络的核心，<strong>它通过卷积核对输入进行卷积操作，从而提取输入的特征</strong>。</p>
<p><strong>池化层</strong></p>
<p>池化层（Pooling Layer）用于<strong>降低特征图的维度</strong>，<strong>提取特征</strong>。</p>
<ul>
<li><strong>最大池化</strong>：取池化窗口中的最大值。</li>
<li>平均池化：取池化窗口中的平均值。</li>
</ul>
<p><strong>全连接层</strong></p>
<p>全连接层（Fully Connected Layer）用于<strong>将卷积层和池化层提取的特征进行组合</strong>，<strong>得到最终的输出</strong>。</p>
<p><strong>卷积神经网络的结构：</strong></p>
<ul>
<li><p><strong>输入层</strong>：接收输入数据，如图像。</p>
<p>  <strong>for loop:</strong></p>
<ul>
<li><strong>卷积层</strong>：使用一组可学习的滤波器（或称为卷积核）对输入数据进行卷积操作，提取出局部特征。</li>
<li><strong>激活层</strong>：对卷积层的输出应用一个非线性函数，如ReLU。</li>
<li><strong>池化层</strong>：进行下采样操作，减少数据的空间大小，从而减少计算量。</li>
</ul>
</li>
<li><p><strong>全连接层</strong>：将学习到的局部特征整合起来，进行最终的分类或回归。</p>
</li>
</ul>
<p><em>其它深度学习应用将在其它文章中详解。</em></p>

</article>
    
    

</div>
<div class="trm-post-next-prev row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2023/10/17/hello-world/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="/img/block.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" /categories/TEST/">
                    TEST
                </a>
            </div>
            <h5>
                <a href="/2023/10/17/hello-world/" class="trm-anima-link">
                    Hello World
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>23/10/17</li>
                <li>12:57</li>
                
                    <li>75</li>
                
                
                    <li>1</li>
                
            </ul>
        </div>
    </div>
</div>
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-scroll-animation">

    

    
        <div class="trm-footer-item">
            <span>© 2023- 2023</span>
            <span class="footer-separator"data-separator=" · "></span>
            <span class="trm-accent-color">浮芒Friman</span>
        </div>
    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.1.4
            </span>
        </div>
      

    
        <div class="trm-footer-item">
            我诞生于 <span id="since" class="trm-accent-color"></span> 天前
        </div>
     

    
        <div class="trm-footer-item">
            这里是页脚的自定义文本
        </div>
     
</footer>

<script>
    function show_date_time () {
        var BirthDay = new Date("10/18/2023 17:00:00");
        var today = new Date();
        var timeold = (today.getTime() - BirthDay.getTime());
        var msPerDay = 24 * 60 * 60 * 1000
        var day = Math.floor(timeold / msPerDay)
        since.innerHTML = day
    }
    show_date_time()
</script>
 
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            

            
<div class="trm-fixed-container">
    
    
        <div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()">
            <i class="iconfont fas fa-book-reader"></i>
        </div>
    
    
    <div id="trm-back-top" class="trm-fixed-btn" data-title="回到顶部">
        <i class="iconfont fas fa-arrow-up"></i>
    </div>
</div>
        </div>
      </div>
      <!-- scroll container end -->
  </div>
  <!-- app wrapper end -->

  
  <!-- Plugin -->




    
    
<script src="https://npm.elemecdn.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    
        <script src="/js/plugins/typing.js?v=2.1.4"></script>
    

    

    <!-- 数学公式 -->
    
        
<script src="https://npm.elemecdn.com/katex@latest/dist/katex.min.js" data-swup-reload-script></script>

        
            
<script src="https://npm.elemecdn.com/katex@latest/dist/contrib/copy-tex.min.js" data-swup-reload-script></script>

        
        
<script src="https://npm.elemecdn.com/katex@latest/dist/contrib/auto-render.min.js" data-swup-reload-script></script>

        <script data-swup-reload-script>
              window.renderMathInElement(document.body, {
                delimiters: [
                    { left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    { left: '\\(', right: '\\)', display: false },
                    { left: '\\[', right: '\\]', display: true },
                ],
                ...{},
            })
        </script>
    

    <!-- 评论插件 -->
    
        

        
    



<!-- CDN -->


    

    

    




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.1.4"></script>

</body>

</html>